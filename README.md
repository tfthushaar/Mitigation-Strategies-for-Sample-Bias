To address sample bias in our machine learning models, we tested four distinct bias mitigation strategies in this study:
SMOTE, in-processing, hybrid approach, and random sampling. Accuracy and disparities in racial and gender demographic parity were used to evaluate each approach.

SMOTE: Accuracy of 61.27% was attained. Significant racial bias was shown by the relatively low demographic parity difference for sex (5.40%) and the considerable demographic parity difference for race (30.57%).

In-Processing Method: At 84.72%, this method produced the best accuracy. However, there was considerable bias in both categories as evidenced by the demographic parity differences of 17.01% for sex and 23.22% for race.

Hybrid Method: 84.12% accuracy was attained. The racial and sexual demographic parity differences were 20.8% and 17.7%, respectively, indicating that bias remained after applying several debiasing techniques.

Random Sampling: An accuracy of 79.21% was obtained using this method. It showed a fair reduction of bias across both sensitive traits, with the lowest demographic parity difference for sex (5.36%) and a moderate demographic parity difference for race (11.02%).

The results showed that while the in-processing method had the best accuracy (84.72%), it still showed some bias in terms of the disparities in demographic parity across sex and race. Although the accuracy of the hybrid approach was 84.12%, bias was not considerably decreased.

Random sampling emerged as a balanced approach, showed the lowest demographic parity gap for sex (5.36%) and a moderate reduction in racial prejudice (11.02%), while obtaining a relatively high accuracy (79.21%).
